{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (1.78.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.5)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.13.0.92)\n",
      "Requirement already satisfied: numpy>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from opencv-python) (2.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D,Activation,LeakyReLU,BatchNormalization,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1623177821157,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "0Zqy61__Rn5L"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer,MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'PlantVillage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m root_dir =  \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPlantVillage\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m listdir()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified: 'PlantVillage'"
     ]
    }
   ],
   "source": [
    "root_dir =  r'PlantVillage'\r\n",
    "\r\n",
    "os.chdir(root_dir)\r\n",
    "listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1623177823254,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "GZ93_IycTZPa"
   },
   "outputs": [],
   "source": [
    "# Dimension of resized image\n",
    "DEFAULT_IMAGE_SIZE = tuple((256, 256))\n",
    "\n",
    "# Number of images used to train the model\n",
    "N_IMAGES = 100\n",
    "\n",
    "data_dir = '.'\n",
    "\n",
    "\"\"\"We use the function `convert_image_to_array` to resize an image\n",
    "to the size `DEFAULT_IMAGE_SIZE` we defined above.\"\"\"\n",
    "\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, DEFAULT_IMAGE_SIZE)\n",
    "            return img_to_array(image)\n",
    "        else:\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1623177825147,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "QIXPhJmoTvyv",
    "outputId": "ba34dd43-b38c-4d19-dddc-40a942cc26e5"
   },
   "outputs": [],
   "source": [
    "print(\"Load images from all classes ...\")\r\n",
    "plant_disease_folder_list = listdir(data_dir)\r\n",
    "print(len(plant_disease_folder_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Training & Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "label_list = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "\n",
    "for class_name in os.listdir('.'):\n",
    "    if not os.path.isdir(class_name):\n",
    "        continue\n",
    "\n",
    "    print(\"Processing:\", class_name)\n",
    "\n",
    "    for img in os.listdir(class_name):\n",
    "        img_path = os.path.join(class_name, img)\n",
    "\n",
    "        try:\n",
    "            img_arr = cv2.imread(img_path)\n",
    "            if img_arr is None:\n",
    "                continue\n",
    "\n",
    "            img_arr = cv2.resize(img_arr, (256, 256))\n",
    "            image_list.append(img_arr)\n",
    "            label_list.append(class_name)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(\"Done loading images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1623177838050,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "Cu2DGtVGW3r3",
    "outputId": "aebba22f-7a88-4210-88f1-43597cc48156"
   },
   "outputs": [],
   "source": [
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11984,
     "status": "ok",
     "timestamp": 1623177852520,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "AgXTKzClTjLY",
    "outputId": "f9aafeb2-f6c1-477f-dbfb-2a0dd52bf1be"
   },
   "outputs": [],
   "source": [
    "# Transform the loaded training image data into numpy array\r\n",
    "np_image_list = np.array(image_list, dtype=np.float16) / 255.0\r\n",
    "\r\n",
    "# Check the number of images loaded for training\r\n",
    "image_len = len(image_list)\r\n",
    "print(f\"Total number of images: {image_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIX: make sure labels are valid and flat ---\n",
    "import numpy as np\n",
    "\n",
    "print(\"Images loaded:\", len(image_list))\n",
    "print(\"Labels loaded:\", len(label_list))\n",
    "\n",
    "# remove any None or empty labels (safety)\n",
    "label_list = [lbl for lbl in label_list if lbl is not None]\n",
    "\n",
    "# convert to numpy array (required by LabelBinarizer)\n",
    "label_list = np.array(label_list)\n",
    "\n",
    "print(\"Clean labels:\", label_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "image_labels = label_binarizer.fit_transform(label_list)\n",
    "\n",
    "n_classes = len(label_binarizer.classes_)\n",
    "print(\"Total number of classes:\", n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1623177852521,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "aofjXWyxTjCq"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Augment and Split Dataset\n",
    "Using `ImageDataGenerator` to augment data by performing various operations on the training images.\n",
    "\"\"\"\n",
    "\n",
    "augment = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                             height_shift_range=0.1, shear_range=0.2, \n",
    "                             zoom_range=0.2, horizontal_flip=True, \n",
    "                             fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1623177853140,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "lQglwnzGYPA5",
    "outputId": "97d37b61-bf7a-4015-e0ea-5561fe1c54ab"
   },
   "outputs": [],
   "source": [
    "\"\"\"Splitting the data into training and test sets for validation purpose.\"\"\"\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42)\r\n",
    "print('Successfully split data into TRAIN & TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6045,
     "status": "ok",
     "timestamp": 1623177873583,
     "user": {
      "displayName": "Akella Niranjan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS0hDW7cYrcLRQ7IP_TVEsKTd1iwuhmSjd-QBh=s64",
      "userId": "10548223611542786095"
     },
     "user_tz": -330
    },
    "id": "a6CiKlFzrIV2",
    "outputId": "e950556d-0b24-419e-a187-72af400f6884"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Build Model\r\n",
    "Defining the hyperparameters of the plant disease classification model.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "EPOCHS = 10\r\n",
    "STEPS = 100\r\n",
    "LR = 1e-3\r\n",
    "BATCH_SIZE = 32\r\n",
    "WIDTH = 256\r\n",
    "HEIGHT = 256\r\n",
    "DEPTH = 3\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D,\n",
    "    Flatten, Dense, Dropout, Activation\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating a Sequential Model to build CNN for multi-class classification\"\"\"\r",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "DEPTH = 3\n",
    "\r\n",
    "model = Sequential()\r\n",
    "inputShape = (HEIGHT, WIDTH, DEPTH)\r\n",
    "chanDim = -1\r\n",
    "\r\n",
    "if K.image_data_format() == \"channels_first\":\r\n",
    "    inputShape = (DEPTH, HEIGHT, WIDTH)\r\n",
    "    chanDim = 1\r\n",
    "\r\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\r\n",
    "model.add(LeakyReLU(alpha=0.1))\r\n",
    "model.add(BatchNormalization(axis=chanDim))\r\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\r\n",
    "model.add(Dropout(0.25))\r\n",
    "\r\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\r\n",
    "model.add(LeakyReLU(alpha=0.1))\r\n",
    "model.add(BatchNormalization(axis=chanDim))\r\n",
    "\r\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\r\n",
    "model.add(LeakyReLU(alpha=0.1))\r\n",
    "model.add(BatchNormalization(axis=chanDim))\r\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "model.add(Dropout(0.25))\r\n",
    "\r\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\r\n",
    "model.add(LeakyReLU(alpha=0.1))\r\n",
    "model.add(BatchNormalization(axis=chanDim))\r\n",
    "\r\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\r\n",
    "model.add(LeakyReLU(alpha=0.1))\r\n",
    "model.add(BatchNormalization(axis=chanDim))\r\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "model.add(Dropout(0.25))\r\n",
    "\r\n",
    "model.add(Flatten())\r\n",
    "\r\n",
    "model.add(Dense(1024, name = 'my_dense'))\r\n",
    "model.add(LeakyReLU(alpha=0.1))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Dropout(0.5))\r\n",
    "\r\n",
    "model.add(Dense(n_classes))\r\n",
    "model.add(Activation(\"softmax\"))\r\n",
    "\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from the Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"my_dense\"\n",
    "\n",
    "intermediate_layer_model = Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=model.get_layer(layer_name).output\n",
    ")\n",
    "\n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "LR = 0.001   \n",
    "\n",
    "opt = Adam(learning_rate=LR)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=opt,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Training CNN...\")\n",
    "\n",
    "history = model.fit(\n",
    "    augment.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {scores[1] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Evaluate Model\r\n",
    "Comparing the accuracy and loss by plotting the graph for training and validation.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "acc = history.history['accuracy']\r\n",
    "val_acc = history.history['val_accuracy']\r\n",
    "loss = history.history['loss']\r\n",
    "val_loss = history.history['val_loss']\r\n",
    "epochs = range(1, len(acc) + 1)\r\n",
    "\r\n",
    "# Train and validation accuracy\r\n",
    "plt.plot(epochs, acc, 'b', label='Training accurarcy')\r\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\r\n",
    "plt.title('Training and Validation accurarcy')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "\r\n",
    "# Train and validation loss\r\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\r\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\r\n",
    "plt.title('Training and Validation loss')\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\"\"\"Evaluating model accuracy by using the `evaluate` method\"\"\"\r\n",
    "\r\n",
    "print(\"[INFO] Calculating model accuracy\")\r\n",
    "scores = model.evaluate(x_test, y_test)\r\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model\r\n",
    "\r\n",
    "# model.save('PDD_completemodel')\r\n",
    "# model.save('PDD_completemodel.h5')\r\n",
    "# intermediate_layer_model.save('PDD_IntermediateModel')\r\n",
    "# intermediate_layer_model.save('PDD_IntermediateModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr2hsdio8hkz"
   },
   "outputs": [],
   "source": [
    "x_train_predict = intermediate_layer_model.predict(x_train)\r\n",
    "# print(x_train_predict.shape)\r\n",
    "\r\n",
    "x_test_predict = intermediate_layer_model.predict(x_test)\r\n",
    "print(x_test_predict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\r\n",
    "\r\n",
    "Integration of CNN with Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm = SVC(kernel='rbf')\r\n",
    "\r\n",
    "svm.fit(x_train_predict,np.argmax(y_train,axis=1))\r\n",
    "\r\n",
    "print('SVM Fit Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(x_train_predict,np.argmax(y_train,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(x_test_predict,np.argmax(y_test,axis=1))\r\n",
    "\r\n",
    "#Save the SVM model in pickle file\r\n",
    "# pickle.dump(svm,open('svms.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_labels = svm.predict(x_test_predict)\r\n",
    "Pred_labels = pd.DataFrame(Pred_labels,index =None)\r\n",
    "Pred_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Integration\r\n",
    "\r\n",
    "Integrating the CNN model with Extreme Gradient Boost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\r\n",
    "\r\n",
    "xb = xgb.XGBClassifier(use_label_encoder=False)\r\n",
    "\r\n",
    "xb.fit(x_train_predict,np.argmax(y_train,axis=1))\r\n",
    "\r\n",
    "print('XGBoost Fit Complete')\r\n",
    "\r\n",
    "#Save XGBoost Model\r\n",
    "# pickle.dump(xb,open('xgb_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.score(x_train_predict,np.argmax(y_train,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.score(x_test_predict,np.argmax(y_test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\r\n",
    "\r\n",
    "# ai = load_model('D:\\PDD_Models\\HDF5\\PDD_Intermediatemodel.h5')\r\n",
    "# xgb_model = xgb.XGBClassifier()\r\n",
    "# xgb_model.load_model(r'D:\\PDD_Models\\xgb_model.json')\r\n",
    "\r\n",
    "# x2 = ai.predict(x_test)\r\n",
    "# xgb_model.score(x_test_predict,np.argmax(y_test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== SAVE MODELS ==================\n",
    "\n",
    "# Save CNN model\n",
    "model.save(\"plant_disease_cnn.h5\")\n",
    "\n",
    "# Save XGBoost model\n",
    "import pickle\n",
    "pickle.dump(xb, open(\"plant_disease_xgb.pkl\", \"wb\"))\n",
    "\n",
    "print(\"Models saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load CNN model\n",
    "model = load_model(\"plant_disease_cnn.h5\")\n",
    "print(\"CNN model loaded\")\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"Apple___Apple_scab\",\n",
    "    \"Apple___Black_rot\",\n",
    "    \"Apple___Cedar_apple_rust\",\n",
    "    \"Cherry_(including_sour)___Powdery_mildew\",\n",
    "    \"Grape___Black_rot\",\n",
    "    \"Peach___Bacterial_spot\",\n",
    "    \"Strawberry___Leaf_scorch\",\n",
    "    \"Tomato___Leaf_Mold\"\n",
    "]\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "def predict_disease(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found\")\n",
    "\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    preds = model.predict(img)\n",
    "    idx = np.argmax(preds)\n",
    "\n",
    "    return CLASS_NAMES[idx], preds[0][idx] * 100\n",
    "\n",
    "# TEST\n",
    "disease, confidence = predict_disease(\n",
    "    r\"C:\\Users\\Lenovo\\InstallConda\\Detection-of-Plant-Disease-main\\test_images\\rotten.jpg\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ¦  Disease:\", disease)\n",
    "print(\"ðŸŽ¯ Confidence:\", round(confidence, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CNN model\n",
    "model.save(\"plant_disease_cnn.h5\")\n",
    "\n",
    "# Save Label Binarizer\n",
    "import pickle\n",
    "pickle.dump(label_binarizer, open(\"plant_disease_label_transform.pkl\", \"wb\"))\n",
    "\n",
    "print(\"Models saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMYnTMjyEbKj3D0r2HxMd9C",
   "name": "Plant_Disease_Detection.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "64c1db036c245e91d1c2d164e41aef3f7526caa4d16da4bd1475087c34dc1c7b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
